## 이현수

## 1. 캐시 메모리란?

> **CPU와 메인 메모리(RAM) 사이에서 데이터 접근 속도를 향상시키기 위해 사용되는 고속 메모리**
> 
- CPU는 작업을 수행할 때 필요한 데이터를 **메인 메모리에서 가져와야 함**
- 하지만 메인 메모리의 속도가 CPU보다 **훨씬 느려** CPU의 성능을 저하시킬 수 있음
- **자주 사용되는 데이터를 미리 캐시 메모리에 저장하여 CPU가 빠르게 접근할 수 있도록 함**

### 캐시 메모리의 위치

![image](https://github.com/user-attachments/assets/cc5b04ca-4e37-4b45-8508-c5c03edfd707)

- **CPU 내부 또는 외부**에 존재할 수 있음
- 일반적으로 **L1, L2는 CPU 내부, L3는 최근 CPU에서는 내부에 포함됨** (일부 서버용 CPU는 L3를 외부에 배치하기도 함)
- **CPU는 L1 → L2 → L3 순서로 캐시를 탐색함**

---

## 2. 캐시 메모리 성능

### 2.1 Hit Ratio (캐시 적중률)

> CPU가 원하는 데이터가 캐시에서 발견되는 비율
> 

$$
\text{Hit Ratio} = \frac{\text{Hit 횟수}}{\text{Hit 횟수 + Miss 횟수}}
$$

- **캐시 Hit:** CPU가 원하는 데이터가 캐시에 존재할 때 발생
- **캐시 Miss:** CPU가 원하는 데이터가 캐시에 없을 때 발생

### 2.2 캐시 Miss의 종류

1. **Compulsory Miss (필수 미스)**: 해당 데이터가 **처음 캐시에 로드될 때** 발생
2. **Conflict Miss (충돌 미스)**: 서로 다른 데이터가 **같은 캐시 블록을 공유해야 할 때** 발생
3. **Capacity Miss (용량 미스)**: 캐시 공간이 부족하여 **기존 데이터를 교체해야 할 때** 발생

---

## 3. 지역성(Locality)

> CPU가 자주 사용할 것으로 예상되는 일부 데이터를 캐시에 저장하는 원리
> 

### 3.1 시간 지역성 (Temporal Locality)

- **한 번 참조된 데이터는 잠시 후 다시 참조될 가능성이 높음**
- **예시:** 반복문 실행

### 3.2 공간 지역성 (Spatial Locality)

- **참조된 데이터 근처에 있는 데이터가 잠시 후 사용될 가능성이 높음**
- **예시:** 배열 순회

### 3.3 캐시 라인 (Cache Line)

- CPU는 데이터를 바이트 단위가 아니라, **32~128바이트 크기의 캐시 라인 단위**로 가져옴
- 특정 데이터 요청 시, **해당 주소가 포함된 캐시 라인을 함께 저장하여 공간 지역성을 활용**

### 3.4 캐시 블록 크기와 성능

- 캐시 블록 크기가 크면 **공간 지역성 활용 가능 → 성능 향상 가능**
- 하지만 **너무 크면 불필요한 데이터까지 로드 → 성능 저하 가능**
- **적절한 블록 크기를 선택하는 것이 중요**

---

## 4. 캐시 매핑 방식

> 메모리 주소를 캐시에 저장하는 방법
> 

### 4.1 Direct Mapped Cache (직접 매핑)

![image (1)](https://github.com/user-attachments/assets/ae70753f-9cce-4a65-b0d9-3109a688e87b)

- **각 메모리 블록이 캐시의 한 블록에만 대응**
- 구조가 단순하고 구현이 쉬움
- **단점:** 특정 메모리 블록들이 같은 캐시 블록을 공유해야 하므로 **Conflict Miss 발생 가능**

### 4.2 Fully Associative Cache (완전 연관 매핑)

![image (2)](https://github.com/user-attachments/assets/aecd56c2-5c50-4b83-a2ce-fb5d37b21e79)

- **캐시의 모든 블록이 임의의 메모리 블록을 저장할 수 있음**
- **Conflict Miss가 없음** → 높은 Hit율 제공
- **단점:** 모든 블록을 검색해야 하므로 Hit 시간이 길어지고, **하드웨어 오버헤드가 큼**

### 4.3 N-Way Set Associative Cache (N-웨이 집합 연관 매핑)

![image (3)](https://github.com/user-attachments/assets/c7f749c5-c88b-4aa2-8749-485579121ffc)

- **Direct Mapped와 Fully Associative의 절충안**
- 캐시를 **N개의 그룹(집합)으로 나누고, 각 집합 내에서 Fully Associative 방식으로 동작**
- **장점:** Direct Mapped보다 높은 Hit율, Conflict Miss 발생률 낮음
- **단점:** N개의 블록을 검색해야 하므로 **Hit 시간이 증가**하며, 하드웨어 비용이 증가할 수 있음

---

## 5. 캐시 쓰기 정책 (Write Policy)

> 캐시에서 데이터를 수정할 때, 메인 메모리에 언제 반영할지 결정하는 방법
> 

### 5.1 Write-through (즉시 기록)

- **데이터를 캐시와 메모리에 동시에 저장**
- **데이터 일관성 유지 가능**
- **단점:** 모든 쓰기 연산이 메모리에 반영되어 **속도가 느림**

### 5.2 Write-back (지연 기록)

- **데이터를 캐시에만 저장하고, 이후에 메모리에 반영**
- **성능 향상 가능**
- **단점:** 캐시와 메모리 간 **데이터 불일치 문제 발생 가능** → 정합성을 유지하는 추가적인 정책 필요

---

## 6. 캐시 교체 알고리즘 (Cache Replacement Policy)

> 캐시가 가득 찼을 때, 어떤 데이터를 제거하고 새로운 데이터를 저장할지 결정하는 방법
> 

### 6.1 FIFO (First In First Out)

![image (4)](https://github.com/user-attachments/assets/bcf30217-d3fc-4918-a057-dab4c8e46d58)

- **가장 먼저 들어온 데이터를 먼저 제거**
- **단점:** 오래된 데이터가 자주 사용될 경우 Hit율이 저하될 수 있음

### 6.2 LRU (Least Recently Used)

![image (5)](https://github.com/user-attachments/assets/f209eebc-4aee-4d38-bf3d-a3e04a40dbb0)

- **가장 오랫동안 사용되지 않은 데이터를 제거**
- **지역성 원리에 적합**하여 실제 시스템에서 많이 사용됨
- **단점:** 각 블록의 사용 시간을 추적해야 하므로 **하드웨어 비용 증가**

### 6.3 LFU (Least Frequently Used)

![image (6)](https://github.com/user-attachments/assets/3c69c937-5f64-49cc-8ee3-f30aa57a497a)

- **가장 적게 사용된 데이터를 제거**
- **단점:** 최근에 많이 사용된 데이터라도 빈도가 낮으면 제거될 가능성이 있음

### 6.4 Random (무작위 교체)

- **임의의 데이터를 선택하여 교체**
- **장점:** 구현이 단순하고, 일부 상황에서는 LRU보다 성능이 나을 수도 있음
- **단점:** Hit율이 낮아질 가능성이 있음

---

## 7. 결론

- **캐시 메모리는 CPU와 메인 메모리 간 속도 차이를 줄이기 위한 중요한 기술**
- **Hit Ratio를 높이는 것이 캐시 설계의 핵심 목표**
- **지역성을 적극 활용하여 성능을 최적화**하는 것이 중요
- **적절한 매핑 방식, 쓰기 정책, 교체 알고리즘을 선택하는 것이 시스템 성능에 큰 영향을 미침**
